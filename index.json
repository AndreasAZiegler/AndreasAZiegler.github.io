[{"authors":null,"categories":null,"content":"I’m currently interning at Sony AI Zürich as a research scientist.\nI am a PhD candidate at the Cognitive Systems Group at the University of Tübingen in collaboration with Sony AI Zürich working on event-based vision for fast robot control. In 2022, I did a PhD internship at Prophesee, supervised by Dr. Amos Sironi.\nI am passioned about a mix of robotics and computer vision research and industrial/commercial applications. My vision is to develop novel algorithms and make them work on real robots.\nBefore I started my PhD studies, I worked as a Robotics Engineer for a few years. Previously, I was a Research Assistant at the Robotics and Perception Group continuing the project of my Master thesis. Before that, I was a Research Associate Intern at Disney Research Zurich. I have done my Master thesis at the Robotics and Perception Group, was a Computer Vision \u0026amp; Robotics intern at Pix4D while I was a master student at D-ITET at ETH Zurich focusing on Computer Vision, Robotics and Machine Learning. Before my master studies, I worked at the Research in Orthopedic Computer Science (ROCS) and at the Institute of Biomechanics after I completed my BSc in Electrical Engineering at the University of Applied Sciences of Eastern Switzerland (FHO).\nDownload my short CV or my long CV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m currently interning at Sony AI Zürich as a research scientist.\nI am a PhD candidate at the Cognitive Systems Group at the University of Tübingen in collaboration with Sony AI Zürich working on event-based vision for fast robot control.","tags":null,"title":"Andreas Ziegler","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://andreasaziegler.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Andreas Ziegler","Thomas Gossard","Karl Vetter","Jonas Tebbe","Andreas Zell"],"categories":null,"content":"","date":1698537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698537600,"objectID":"3b49f67355a2f0789d2af161b5ba0531","permalink":"https://andreasaziegler.github.io/publication/corlw23-ziegler/","publishdate":"2023-10-29T00:00:00Z","relpermalink":"/publication/corlw23-ziegler/","section":"publication","summary":"In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.","tags":["Robotics","Computer Vision","Camera calibration","SNN","Spin Detection"],"title":"A multi-modal table tennis robot system","type":"publication"},{"authors":["Thomas Gossard","Andreas Ziegler","Levin Komar","Jonas Tebbe","Andreas Zell"],"categories":null,"content":"","date":1695369077,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695369077,"objectID":"8fcd51a9a104bf73ac1a43552e809a16","permalink":"https://andreasaziegler.github.io/publication/arxiv-gossard/","publishdate":"2023-09-23T00:00:00Z","relpermalink":"/publication/arxiv-gossard/","section":"publication","summary":"Accurate calibration is crucial for using multiple cameras to triangulate the position of objects precisely. However, it is also a time-consuming process that needs to be repeated for every displacement of the cameras. The standard approach is to use a printed pattern with known geometry to estimate the intrinsic and extrinsic parameters of the cameras. The same idea can be applied to event-based cameras, though it requires extra work. By using frame reconstruction from events, a printed pattern can be detected. A blinking pattern can also be displayed on a screen. Then, the pattern can be directly detected from the events. Such calibration methods can provide accurate intrinsic calibration for both frame- and event-based cameras. However, using 2D patterns has several limitations for multi-camera extrinsic calibration, with cameras possessing highly different points of view and a wide baseline. The 2D pattern can only be detected from one direction and needs to be of significant size to compensate for its distance to the camera. This makes the extrinsic calibration time-consuming and cumbersome. To overcome these limitations, we propose eWand, a new method that uses blinking LEDs inside opaque spheres instead of a printed or displayed pattern. Our method provides a faster, easier-to-use extrinsic calibration approach that maintains high accuracy for both event- and frame-based cameras.","tags":["Robotics","Computer Vision","Camera calibration"],"title":"eWand: A calibration framework for wide baseline frame-based and event-based camera systems","type":"publication"},{"authors":null,"categories":null,"content":"","date":1682035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682035200,"objectID":"8201f06267650da4cd6d84e27d095e87","permalink":"https://andreasaziegler.github.io/teaching/agnn/","publishdate":"2023-04-21T00:00:00Z","relpermalink":"/teaching/agnn/","section":"teaching","summary":"The goal of this thesis is to apply these Graph-based networks for ball detection with event cameras.","tags":["Robotics","Computer Vision","Event Camera","Deep Learning"],"title":"Asynchronous Graph-based Neural Networks for Ball Detection with Event Cameras","type":"teaching"},{"authors":null,"categories":null,"content":"","date":1682035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682035200,"objectID":"3aa6603a698a8a236b894cd5cf9d4c78","permalink":"https://andreasaziegler.github.io/teaching/mot/","publishdate":"2023-04-21T00:00:00Z","relpermalink":"/teaching/mot/","section":"teaching","summary":"The goal of this thesis is to develop a real-time capable (multi) object tracking pipeline by applying multi object segmentation.","tags":["Robotics","Computer Vision","Event Camera","Deep Learning"],"title":"Multi Object tracking via event-based motion segmentation with event cameras","type":"teaching"},{"authors":["Thomas Gossard","Jonas Tebbe","Andreas Ziegler","Andreas Zell"],"categories":null,"content":"","date":1678147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678147200,"objectID":"98bdccb6002a02329cec9a2ebdd24203","permalink":"https://andreasaziegler.github.io/publication/iros23-gossard/","publishdate":"2023-03-07T00:00:00Z","relpermalink":"/publication/iros23-gossard/","section":"publication","summary":"Spin plays a considerable role in table tennis, making a shot’s trajectory harder to read and predict. However, the spin is challenging to measure because of the ball’s high velocity and the magnitude of the spin values. Existing methods either require extremely high framerate cameras or are unreliable because they use the ball’s logo, which may not always be visible. Because of this, many table tennis-playing robots ignore the spin, which severely limits their capabilities. This paper proposes an easily implementable and reliable spin estimation method. We developed a dotted-ball orientation estimation (DOE) method, that can then be used to estimate the spin. The dots are first localized on the image using a CNN and then identified using geometric hashing. The spin is finally regressed from the estimated orientations. Using our algorithm, the ball’s orientation can be estimated with a mean error of 2.4° and the spin estimation has an relative error lower than 1%. Spins up to 175 rps are measurable with a camera of 350 fps in real time. Using our method, we generated a dataset of table tennis ball trajectories with position and spin, available on our project page.","tags":["Robotics","Table tennis","Spin estimation","Real-time"],"title":"SpinDOE: A ball spin estimation method for table tennis robot","type":"publication"},{"authors":null,"categories":null,"content":"Abstract In exploration, the goal is to build a map of an unknown environment. Most state-of-the-art approaches use map representations that require drift-free state estimates to function properly. Real-world state estimators, however, exhibit drift. In this paper, we present a 2D map representation for exploration that is robust to drift. Rather than a global map, it uses local metric volumes connected by relative pose estimates. This pose-graph does not need to be globally consistent. Overlaps between the volumes are resolved locally, rather than on the faulty estimate of space. We demonstrate our representation with a frontier-based exploration approach, evaluate it under different conditions and compare it with a commonly-used grid-based representation. We show that, at the cost of longer exploration time, using the proposed representation allows full coverage of space even for very large drift in the state estimate, contrary to the grid-based representation. The system is validated in a real world experiment and we discuss its extension to 3D.\n","date":1675036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675036800,"objectID":"5ed94e05d71f15755e9717d05cbd546a","permalink":"https://andreasaziegler.github.io/project/event-simulator/","publishdate":"2023-01-30T00:00:00Z","relpermalink":"/project/event-simulator/","section":"project","summary":"The goal of this project was to develop an event simulator which can run in real-time given a video stream from a frame-based camera.","tags":["Robotics","Computer Vision"],"title":"A real-time event simulator","type":"project"},{"authors":["Andreas Ziegler","Daniel Teigland","Jonas Tebbe","Thomas Gossard","Andreas Zell"],"categories":null,"content":"","date":1662768e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662768e3,"objectID":"9093d95d7903c509bec767cc17db4113","permalink":"https://andreasaziegler.github.io/publication/icra23-ziegler/","publishdate":"2022-12-08T00:00:00Z","relpermalink":"/publication/icra23-ziegler/","section":"publication","summary":"Event cameras are becoming increasingly popular in robotics and computer vision due to their beneficial properties, e.g., high temporal resolution, high bandwidth, almost no motion blur, and low power consumption. However, these cameras remain expensive and scarce in the market, making them inaccessible to the majority. Using event simulators minimizes the need for real event cameras to develop novel algorithms. However, due to the computational complexity of the simulation, the event streams of existing simulators cannot be generated in real-time but rather have to be pre-calculated from existing video sequences or pre-rendered and then simulated from a virtual 3D scene. Although these offline generated event streams can be used as training data for learning tasks, all response time dependent applications cannot benefit from these simulators yet, as they still require an actual event camera. This work proposes simulation methods that improve the performance of event simulation by two orders of magnitude (making them real-time capable) while remaining competitive in the quality assessment.","tags":["Robotics","Event-based Computer Vision","Real-time"],"title":"Real-time event simulation with frame-based cameras","type":"publication"},{"authors":null,"categories":null,"content":"Description Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μs), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision.\nSo far, most learning approaches applied to event data, convert a batch of events into a tensor and then use conventional CNNs as network. While such approaches achieve state-of-the-art performance, they do not make use of the asynchronous nature of the event data. Spiking Neural Networks (SNNs) on the other hand are bio-inspired networks that can process output from event-based directly. SNNs process information conveyed as temporal spikes rather than numeric values. This makes SNNs an ideal counterpart for event-based cameras.\nThe goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls. The Cognitive Systems groups has a table tennis robot system, where the developed ball tracker can be used and compared to other methods.\nRequirements Familiar with “traditional” Computer Vision Deep Learning Python ","date":1613865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613865600,"objectID":"b764d679328048edb910c4c05c75237f","permalink":"https://andreasaziegler.github.io/teaching/scnn/","publishdate":"2021-02-21T00:00:00Z","relpermalink":"/teaching/scnn/","section":"teaching","summary":"The goal of this thesis is to use Asynchronous Sparse Convolutional Layers and apply it in a neural network to detect fast moving table tennis balls in real-time.","tags":["Robotics","Computer Vision","Event Camera","Deep Learning"],"title":"Ball Detection with event-based asynchronous sparse convolutional networks","type":"teaching"},{"authors":null,"categories":null,"content":"Description Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μs), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision.\nSo far, most learning approaches applied to event data, convert a batch of events into a tensor and then use conventional CNNs as network. While such approaches achieve state-of-the-art performance, they do not make use of the asynchronous nature of the event data. Spiking Neural Networks (SNNs) on the other hand are bio-inspired networks that can process output from event-based directly. SNNs process information conveyed as temporal spikes rather than numeric values. This makes SNNs an ideal counterpart for event-based cameras.\nThe goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls. The Cognitive Systems groups has a table tennis robot system, where the developed ball tracker can be used and compared to other methods.\nRequirements Familiar with “traditional” Computer Vision Deep Learning Python ","date":1613865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613865600,"objectID":"d9b6dc05d6c0eda116655407974662ea","permalink":"https://andreasaziegler.github.io/teaching/snn/","publishdate":"2021-02-21T00:00:00Z","relpermalink":"/teaching/snn/","section":"teaching","summary":"The goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls.","tags":["Robotics","Computer Vision","Event Camera","Deep Learning","Spiking Neural Networks"],"title":"Spiking neural network for event-based ball detection","type":"teaching"},{"authors":["Aron N. Horvath","Andreas Ziegler","Stephan Gerhard","Claude N. Holenstein","Benjamin Beyeler","Jess G. Snedeker","Unai Silvan"],"categories":null,"content":"","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"466cee8021ad3801d0e33891d2c1caa1","permalink":"https://andreasaziegler.github.io/publication/bj21-horvath/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bj21-horvath/","section":"publication","summary":"Among the stimuli to which cells are exposed in vivo, it has been shown that tensile deformations induce specific cellular responses in musculoskeletal, cardiovascular, and stromal tissues. However, the early response of cells to sustained substrate-based stretch has remained elusive because of the short timescale at which it occurs. To measure the tensile mechanical properties of adherent cells immediately after the application of substrate deformations, we have developed a dynamic traction force microscopy method that enables subsecond temporal resolution imaging of transient subcellular events. The system employs a novel, to our knowledge, tracking approach with minimal computational overhead to compensate substrate-based, stretch-induced motion/drift of stretched single cells in real time, allowing capture of biophysical phenomena on multiple channels by fluorescent multichannel imaging on a single camera, thus avoiding the need for beam splitting with the associated loss of light. Using this tool, we have characterized the transient subcellular forces and nuclear deformations of single cells immediately after the application of equibiaxial strain. Our experiments reveal significant differences in the cell relaxation dynamics and in the intracellular propagation of force to the nuclear compartment in cells stretched at different strain rates and exposes the need for time control for the correct interpretation of dynamic cell mechanics experiments.","tags":[],"title":"Focus on time: dynamic imaging reveals stretch-dependent cell relaxation and nuclear deformation","type":"publication"},{"authors":["Aron N. Horvath","Andreas Ziegler","Stephan Gerhard","Claude N. Holenstein","Benjamin Beyeler","Jess G. Snedeker","Unai Silvan"],"categories":null,"content":"","date":1583193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583193600,"objectID":"3d6c9305242b9d635899167977c3184f","permalink":"https://andreasaziegler.github.io/publication/biorxiv20-horvath/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/biorxiv20-horvath/","section":"publication","summary":"Here, a dynamic traction force microscopy method is described which enables sub-second temporal resolution imaging of transient subcellular events secondary to extrinsic stretch of adherent single cells. The system employs a novel tracking approach with minimal computational overhead to compensate substrate-based stretch-induced motion/drift of stretched single cells in real time, allowing capture of biophysical phenomena on multiple channels by fluorescent multichannel imaging on a single camera, thus avoiding the need for beam splitting with associated loss of light. The potential impact of the technique is demonstrated by characterizing transient subcellular forces and corresponding nuclear deformations in equibiaxial stretching experiments, uncovering a high frequency strain-rate dependent response in the transfer of substrate strains to the nucleus.","tags":[],"title":"Time-controlled Multichannel Dynamic Traction Imaging of Biaxially Stretched Adherent Cells","type":"publication"},{"authors":["Titus Cieslewski","Andreas Ziegler","Davide Scaramuzza"],"categories":null,"content":"","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567468800,"objectID":"f18d17a046c5bd87f794adcf798721a2","permalink":"https://andreasaziegler.github.io/publication/isrr19-cieslewski/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/isrr19-cieslewski/","section":"publication","summary":"In exploration, the goal is to build a map of an unknown environment. Most state-of-the-art approaches use map representations that require drift-free state estimates to function properly. Real-world state estimators, however, exhibit drift. In this paper, we present a 2D map representation for exploration that is robust to drift. Rather than a global map, it uses local metric volumes connected by relative pose estimates. This pose-graph does not need to be globally consistent. Overlaps between the volumes are resolved locally, rather than on the faulty estimate of space. We demonstrate our representation with a frontier-based exploration approach, evaluate it under different conditions and compare it with a commonly-used grid-based representation. We show that, at the cost of longer exploration time, using the proposed representation allows full coverage of space even for very large drift in the state estimate, contrary to the grid-based representation. The system is validated in a real world experiment and we discuss its extension to 3D.","tags":["Robotics","Mapping","Localization","Exploration"],"title":"Exploration Without Global Consistency Using Local Volume Consolidation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://andreasaziegler.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Motivation Exploration is a fundamental task in the field of robotics. The goal is to build a map of a previously unknown environment. Two tasks have to be performed repeatedly for exploration: mapping the space the robot so far perceived; and planning where to go next. In this thesis we focus on the first task and the goal is to find a map representation that can deal with noisy state estimates. All so far presented map representations either assume perfect state estimates or need to rebuild the map after optimization.\nApproach To achieve this goal we build our representation with polygons. Our polygons represent the boundary between free known space and occupied space or unknown space and the inside of polygons is implicitly free space. We develop two approaches: The first approach builds a global map with polygons by continuously building the union of the polygon from the current field of view and the polygons of the so far explored space. The second approach works with polygons of the local field of view only. For every local polygon the frontiers, the obstacles and the free space is determined and the robot explores as long frontiers are present in the map.\nResult In this thesis we propose a novel representation that can deal with noisy state estimates and does not need to rebuild the map or parts of it, e.g. after a loop closure. By experiments we show that we achieve full coverage of the area to explore with frontier-based exploration, using our proposed representation.\n","date":1525046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525046400,"objectID":"68ae3647c67ca0375162a7b1815362e8","permalink":"https://andreasaziegler.github.io/project/exploration/","publishdate":"2018-04-30T00:00:00Z","relpermalink":"/project/exploration/","section":"project","summary":"The goal of this thesis was to develop a (map) representation for exploration that is robust to state estimate drift.","tags":["Robotics","Mapping","Localization","Exploration","Planning"],"title":"A Representation for Exploration that is Robust to State Estimate Drift","type":"project"},{"authors":null,"categories":null,"content":"Motivation A correct and accurate common map is crucial for multiple robots to collaboratively performing tasks. In this semester project, an existing multi agent Simultaneous Localisation and Mapping (SLAM) system should be extended to fuse maps of single robots in a way that no false map alignment is guaranteed and that an optimal alignment is achieved by using multiple place matches. Also redundant information, a consequence of the fusion of two maps, should be removed in order to get a good performance of the optimization routines e.g. Bundle Adjustment (BA).\nApproach To achieve the first goal, a new approach is proposed which uses multiple KeyFrame Matchs (KFMs) to fuse two maps. This proposed approach also use a novel idea to spread the KFMs over a bigger area by skipping KeyFrames (KFs) between the detection of KFMs. To remove redundant information, KF culling is performed after all KFMs were detected and before the main map fusion. This way information which were present in both maps appear only once in the fused map and the Pose Graph Optimization (PGO) and the BA does not have unnecessary data to process. To reduce the runtime of the optimization part (PGO and BA), the usage of Powell’s dog leg (DL) non-linear least squares technique instead of the Levenberg-Marquardt (LM) optimization was evaluated and the system was adapted in a way, that the performance is increased.\nResult The proposed map fusion approach reduces drift and achieves better accuracy compared to the previous approach. With the implemented KF culling, the number of KFs which the PGO and the BA have to process is decreased significantly and therefore better timing is achieved. The usage of DL non-linear least squares technique reduced the runtime of the optimization furthermore.\n","date":1490832e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490832e3,"objectID":"14bb90f9400fd31ab88aaa0bbe58c2f8","permalink":"https://andreasaziegler.github.io/project/map-fusion/","publishdate":"2017-03-30T00:00:00Z","relpermalink":"/project/map-fusion/","section":"project","summary":"The goal of this project was to develop a pipeline to merge maps created by different Unmanned Aerial Vehicles (UAVs) operating in the same area.","tags":["Computer Vision","Robotics","SLAM","Mapping","Optimiaztion"],"title":"Map Fusion for Collaborative UAV SLAM","type":"project"},{"authors":null,"categories":null,"content":"Motivation Object tracking is an important part for many applications especially for robotic systems interacting with humans. Problem statement Ultra-wideband (UWB) systems as well as vision based object trackers are widely known and used. Both of the systems have their advantages and disadvantages. UWB systems can provide the location of an object in 3D with an accuracy of approximate 10cm whereas vision based object trackers can only provide the location of an object in 2D pixel coordinates but with a more precise accuracy than UWB systems.\nApproach So why not combine these two sources of information? Exactly this concept should be developed and evaluated in this semester project. The 3D position measured by the UWB system should be fused with the 2D pixel coordinates of a visual object tracker with an Extended Kalman Filter (EKF). A re-detection mechanism for the visual tracker should be implemented in addition to increase the usability as well as the stability of the system.\nResult The proposed system shows a significantly better accuracy compared with the 3D positions measured by the UWB system. This proof of concept enables to apply this system to a wide range of applications and also allows further extensions.\n","date":1478044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478044800,"objectID":"a9e952751201f8b9ee7493cb07c2686e","permalink":"https://andreasaziegler.github.io/project/object-tracking/","publishdate":"2016-11-02T00:00:00Z","relpermalink":"/project/object-tracking/","section":"project","summary":"In this semester project a new object tracking approach is proposed. A fusion of Ultra-wideband (UWB) and visual measurements to track an object in 3D by fusing both modalities in a principled manner.","tags":["Computer Vision","Object Tracking","UWB","Sensor Fusion","Kalman Filtering","Tracking"],"title":"Robust object tracking in 3D by fusing ultra-wideband and vision","type":"project"}]