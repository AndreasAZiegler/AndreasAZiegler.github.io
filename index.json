[{"authors":null,"categories":null,"content":"I am a PhD candidate at the Cognitive Systems Group at the University of Tübingen in collaboration with Sony AI Zürich working on event-based vision for fast robot control. In 2022, I did a PhD internship at Prophesee, supervised by Dr. Amos Sironi.\nI am passioned about a mix of robotics and computer vision research and industrial/commercial applications. My vision is to develop novel algorithms and make them work on real robots.\nBefore I started my PhD studies, I worked as a Robotics Engineer for a few years. Previously, I was a Research Assistant at the Robotics and Perception Group continuing the project of my Master thesis. Before that, I was a Research Associate Intern at Disney Research Zurich. I have done my Master thesis at the Robotics and Perception Group, was a Computer Vision \u0026amp; Robotics intern at Pix4D while I was a master student at D-ITET at ETH Zurich focusing on Computer Vision, Robotics and Machine Learning. Before my master studies, I worked at the Research in Orthopedic Computer Science (ROCS) and at the Institute of Biomechanics after I completed my BSc in Electrical Engineering at the University of Applied Sciences of Eastern Switzerland (FHO).\nDownload my resumé.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD candidate at the Cognitive Systems Group at the University of Tübingen in collaboration with Sony AI Zürich working on event-based vision for fast robot control. In 2022, I did a PhD internship at Prophesee, supervised by Dr.","tags":null,"title":"Andreas Ziegler","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://andreasaziegler.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Andreas Ziegler","Daniel Teigland","Jonas Tebbe","Thomas Gossard","Andreas Zell"],"categories":null,"content":"","date":1662768e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662768e3,"objectID":"4d7f08b983c96f3f91a6d5c0e7c95cb2","permalink":"https://andreasaziegler.github.io/publication/arxiv22-ziegler/","publishdate":"2022-12-08T00:00:00Z","relpermalink":"/publication/arxiv22-ziegler/","section":"publication","summary":"Event cameras are becoming increasingly popular in robotics and computer vision due to their beneficial properties, e.g., high temporal resolution, high bandwidth, almost no motion blur, and low power consumption. However, these cameras remain expensive and scarce in the market, making them inaccessible to the majority. Using event simulators minimizes the need for real event cameras to develop novel algorithms. However, due to the computational complexity of the simulation, the event streams of existing simulators cannot be generated in real-time but rather have to be pre-calculated from existing video sequences or pre-rendered and then simulated from a virtual 3D scene. Although these offline generated event streams can be used as training data for learning tasks, all response time dependent applications cannot benefit from these simulators yet, as they still require an actual event camera. This work proposes simulation methods that improve the performance of event simulation by two orders of magnitude (making them real-time capable) while remaining competitive in the quality assessment.","tags":["Robotics","Event-based Computer Vision","Real-time"],"title":"Real-time event simulation with frame-based cameras","type":"publication"},{"authors":null,"categories":null,"content":"Description The cognitive systems group at the University of Tübingen uses a table tennis robot system to conduct research on various topics around robotics, control, computer vision, machine learning and reinforcement learning. So far the system uses up to five cameras for the perception pipeline. Recently, the group added event-based cameras to the sensor suite.\nEvent cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μs), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision.\nWith this new sensors in place, the questions arises, how the whole system should be calibrated. Eye-to-hand (calibration of the camera and the robot arm) and camera calibration is a well studied topic in the literature. However, since event-based cameras are still relatively new, there is only very little literature on event-based camera calibration, especially for eye-to-hand calibration.\nIn a first step, the student should study the state-of-the-art calibration methods relevant for our table tennis robot system. Based on this, the goal of the thesis is to develop new methods which can be used in a calibration toolbox, allowing to calibrate the system in an automatic fashion.\nRequirements Familiar with “traditional” Computer Vision C++ and/or Python ","date":1613865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613865600,"objectID":"9d50efe573ea91787d7e5cc01c24925b","permalink":"https://andreasaziegler.github.io/teaching/calibration/","publishdate":"2021-02-21T00:00:00Z","relpermalink":"/teaching/calibration/","section":"teaching","summary":"The goal of the thesis is to develop new methods which can be used in a calibration toolbox, allowing to calibrate the system in an automatic fashion.","tags":["Robotics","Computer Vision","Event Camera","Calibration"],"title":"Event-camera, camera and robot arm calibration","type":"teaching"},{"authors":null,"categories":null,"content":"Description Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μs), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision.\nSo far, most learning approaches applied to event data, convert a batch of events into a tensor and then use conventional CNNs as network. While such approaches achieve state-of-the-art performance, they do not make use of the asynchronous nature of the event data. Spiking Neural Networks (SNNs) on the other hand are bio-inspired networks that can process output from event-based directly. SNNs process information conveyed as temporal spikes rather than numeric values. This makes SNNs an ideal counterpart for event-based cameras.\nThe goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls. The Cognitive Systems groups has a table tennis robot system, where the developed ball tracker can be used and compared to other methods.\nRequirements Familiar with “traditional” Computer Vision Deep Learning Python ","date":1613865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613865600,"objectID":"d9b6dc05d6c0eda116655407974662ea","permalink":"https://andreasaziegler.github.io/teaching/snn/","publishdate":"2021-02-21T00:00:00Z","relpermalink":"/teaching/snn/","section":"teaching","summary":"The goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls.","tags":["Robotics","Computer Vision","Event Camera","Deep Learning","Spiking Neural Networks"],"title":"Spiking neural network for event-based ball detection","type":"teaching"},{"authors":["Aron N. Horvath","Andreas Ziegler","Stephan Gerhard","Claude N. Holenstein","Benjamin Beyeler","Jess G. Snedeker","Unai Silvan"],"categories":null,"content":"","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"466cee8021ad3801d0e33891d2c1caa1","permalink":"https://andreasaziegler.github.io/publication/bj21-horvath/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bj21-horvath/","section":"publication","summary":"Among the stimuli to which cells are exposed in vivo, it has been shown that tensile deformations induce specific cellular responses in musculoskeletal, cardiovascular, and stromal tissues. However, the early response of cells to sustained substrate-based stretch has remained elusive because of the short timescale at which it occurs. To measure the tensile mechanical properties of adherent cells immediately after the application of substrate deformations, we have developed a dynamic traction force microscopy method that enables subsecond temporal resolution imaging of transient subcellular events. The system employs a novel, to our knowledge, tracking approach with minimal computational overhead to compensate substrate-based, stretch-induced motion/drift of stretched single cells in real time, allowing capture of biophysical phenomena on multiple channels by fluorescent multichannel imaging on a single camera, thus avoiding the need for beam splitting with the associated loss of light. Using this tool, we have characterized the transient subcellular forces and nuclear deformations of single cells immediately after the application of equibiaxial strain. Our experiments reveal significant differences in the cell relaxation dynamics and in the intracellular propagation of force to the nuclear compartment in cells stretched at different strain rates and exposes the need for time control for the correct interpretation of dynamic cell mechanics experiments.","tags":[],"title":"Focus on time: dynamic imaging reveals stretch-dependent cell relaxation and nuclear deformation","type":"publication"},{"authors":["Aron N. Horvath","Andreas Ziegler","Stephan Gerhard","Claude N. Holenstein","Benjamin Beyeler","Jess G. Snedeker","Unai Silvan"],"categories":null,"content":"","date":1583193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583193600,"objectID":"3d6c9305242b9d635899167977c3184f","permalink":"https://andreasaziegler.github.io/publication/biorxiv20-horvath/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/biorxiv20-horvath/","section":"publication","summary":"Here, a dynamic traction force microscopy method is described which enables sub-second temporal resolution imaging of transient subcellular events secondary to extrinsic stretch of adherent single cells. The system employs a novel tracking approach with minimal computational overhead to compensate substrate-based stretch-induced motion/drift of stretched single cells in real time, allowing capture of biophysical phenomena on multiple channels by fluorescent multichannel imaging on a single camera, thus avoiding the need for beam splitting with associated loss of light. The potential impact of the technique is demonstrated by characterizing transient subcellular forces and corresponding nuclear deformations in equibiaxial stretching experiments, uncovering a high frequency strain-rate dependent response in the transfer of substrate strains to the nucleus.","tags":[],"title":"Time-controlled Multichannel Dynamic Traction Imaging of Biaxially Stretched Adherent Cells","type":"publication"},{"authors":["Titus Cieslewski","Andreas Ziegler","Davide Scaramuzza"],"categories":null,"content":"","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567468800,"objectID":"f18d17a046c5bd87f794adcf798721a2","permalink":"https://andreasaziegler.github.io/publication/isrr19-cieslewski/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/isrr19-cieslewski/","section":"publication","summary":"In exploration, the goal is to build a map of an unknown environment. Most state-of-the-art approaches use map representations that require drift-free state estimates to function properly. Real-world state estimators, however, exhibit drift. In this paper, we present a 2D map representation for exploration that is robust to drift. Rather than a global map, it uses local metric volumes connected by relative pose estimates. This pose-graph does not need to be globally consistent. Overlaps between the volumes are resolved locally, rather than on the faulty estimate of space. We demonstrate our representation with a frontier-based exploration approach, evaluate it under different conditions and compare it with a commonly-used grid-based representation. We show that, at the cost of longer exploration time, using the proposed representation allows full coverage of space even for very large drift in the state estimate, contrary to the grid-based representation. The system is validated in a real world experiment and we discuss its extension to 3D.","tags":["Robotics","Mapping","Localization","Exploration"],"title":"Exploration Without Global Consistency Using Local Volume Consolidation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://andreasaziegler.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Motivation Exploration is a fundamental task in the field of robotics. The goal is to build a map of a previously unknown environment. Two tasks have to be performed repeatedly for exploration: mapping the space the robot so far perceived; and planning where to go next. In this thesis we focus on the first task and the goal is to find a map representation that can deal with noisy state estimates. All so far presented map representations either assume perfect state estimates or need to rebuild the map after optimization.\nApproach To achieve this goal we build our representation with polygons. Our polygons represent the boundary between free known space and occupied space or unknown space and the inside of polygons is implicitly free space. We develop two approaches: The first approach builds a global map with polygons by continuously building the union of the polygon from the current field of view and the polygons of the so far explored space. The second approach works with polygons of the local field of view only. For every local polygon the frontiers, the obstacles and the free space is determined and the robot explores as long frontiers are present in the map.\nResult In this thesis we propose a novel representation that can deal with noisy state estimates and does not need to rebuild the map or parts of it, e.g. after a loop closure. By experiments we show that we achieve full coverage of the area to explore with frontier-based exploration, using our proposed representation.\n","date":1525046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525046400,"objectID":"68ae3647c67ca0375162a7b1815362e8","permalink":"https://andreasaziegler.github.io/project/exploration/","publishdate":"2018-04-30T00:00:00Z","relpermalink":"/project/exploration/","section":"project","summary":"The goal of this thesis was to develop a (map) representation for exploration that is robust to state estimate drift.","tags":["Robotics","Mapping","Localization","Exploration","Planning"],"title":"A Representation for Exploration that is Robust to State Estimate Drift","type":"project"},{"authors":null,"categories":null,"content":"Motivation A correct and accurate common map is crucial for multiple robots to collaboratively performing tasks. In this semester project, an existing multi agent Simultaneous Localisation and Mapping (SLAM) system should be extended to fuse maps of single robots in a way that no false map alignment is guaranteed and that an optimal alignment is achieved by using multiple place matches. Also redundant information, a consequence of the fusion of two maps, should be removed in order to get a good performance of the optimization routines e.g. Bundle Adjustment (BA).\nApproach To achieve the first goal, a new approach is proposed which uses multiple KeyFrame Matchs (KFMs) to fuse two maps. This proposed approach also use a novel idea to spread the KFMs over a bigger area by skipping KeyFrames (KFs) between the detection of KFMs. To remove redundant information, KF culling is performed after all KFMs were detected and before the main map fusion. This way information which were present in both maps appear only once in the fused map and the Pose Graph Optimization (PGO) and the BA does not have unnecessary data to process. To reduce the runtime of the optimization part (PGO and BA), the usage of Powell’s dog leg (DL) non-linear least squares technique instead of the Levenberg-Marquardt (LM) optimization was evaluated and the system was adapted in a way, that the performance is increased.\nResult The proposed map fusion approach reduces drift and achieves better accuracy compared to the previous approach. With the implemented KF culling, the number of KFs which the PGO and the BA have to process is decreased significantly and therefore better timing is achieved. The usage of DL non-linear least squares technique reduced the runtime of the optimization furthermore.\n","date":1490832e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490832e3,"objectID":"14bb90f9400fd31ab88aaa0bbe58c2f8","permalink":"https://andreasaziegler.github.io/project/map-fusion/","publishdate":"2017-03-30T00:00:00Z","relpermalink":"/project/map-fusion/","section":"project","summary":"The goal of this project was to develop a pipeline to merge maps created by different Unmanned Aerial Vehicles (UAVs) operating in the same area.","tags":["Computer Vision","Robotics","SLAM","Mapping","Optimiaztion"],"title":"Map Fusion for Collaborative UAV SLAM","type":"project"},{"authors":null,"categories":null,"content":"Motivation Object tracking is an important part for many applications especially for robotic systems interacting with humans. Problem statement Ultra-wideband (UWB) systems as well as vision based object trackers are widely known and used. Both of the systems have their advantages and disadvantages. UWB systems can provide the location of an object in 3D with an accuracy of approximate 10cm whereas vision based object trackers can only provide the location of an object in 2D pixel coordinates but with a more precise accuracy than UWB systems.\nApproach So why not combine these two sources of information? Exactly this concept should be developed and evaluated in this semester project. The 3D position measured by the UWB system should be fused with the 2D pixel coordinates of a visual object tracker with an Extended Kalman Filter (EKF). A re-detection mechanism for the visual tracker should be implemented in addition to increase the usability as well as the stability of the system.\nResult The proposed system shows a significantly better accuracy compared with the 3D positions measured by the UWB system. This proof of concept enables to apply this system to a wide range of applications and also allows further extensions.\n","date":1478044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478044800,"objectID":"a9e952751201f8b9ee7493cb07c2686e","permalink":"https://andreasaziegler.github.io/project/object-tracking/","publishdate":"2016-11-02T00:00:00Z","relpermalink":"/project/object-tracking/","section":"project","summary":"In this semester project a new object tracking approach is proposed. A fusion of Ultra-wideband (UWB) and visual measurements to track an object in 3D by fusing both modalities in a principled manner.","tags":["Computer Vision","Object Tracking","UWB","Sensor Fusion","Kalman Filtering","Tracking"],"title":"Robust object tracking in 3D by fusing ultra-wideband and vision","type":"project"}]