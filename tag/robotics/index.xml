<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Robotics | Andreas Ziegler</title><link>https://andreasaziegler.github.io/tag/robotics/</link><atom:link href="https://andreasaziegler.github.io/tag/robotics/index.xml" rel="self" type="application/rss+xml"/><description>Robotics</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2023 Andreas Ziegler</copyright><lastBuildDate>Fri, 21 Apr 2023 00:00:00 +0000</lastBuildDate><image><url>https://andreasaziegler.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Robotics</title><link>https://andreasaziegler.github.io/tag/robotics/</link></image><item><title>Asynchronous Graph-based Neural Networks for Ball Detection with Event Cameras</title><link>https://andreasaziegler.github.io/teaching/agnn/</link><pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/teaching/agnn/</guid><description/></item><item><title>Multi Object tracking via event-based motion segmentation with event cameras</title><link>https://andreasaziegler.github.io/teaching/mot/</link><pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/teaching/mot/</guid><description/></item><item><title>SpinDOE: A ball spin estimation method for table tennis robot</title><link>https://andreasaziegler.github.io/publication/arxiv23-gossard/</link><pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/publication/arxiv23-gossard/</guid><description/></item><item><title>A real-time event simulator</title><link>https://andreasaziegler.github.io/project/event-simulator/</link><pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/project/event-simulator/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>In exploration, the goal is to build a map of an unknown environment. Most state-of-the-art approaches use map representations that require drift-free state estimates to function properly. Real-world state estimators, however, exhibit drift. In this paper, we present a 2D map representation for exploration that is robust to drift. Rather than a global map, it uses local metric volumes connected by relative pose estimates. This pose-graph does not need to be globally consistent. Overlaps between the volumes are resolved locally, rather than on the faulty estimate of space. We demonstrate our representation with a frontier-based exploration approach, evaluate it under different conditions and compare it with a commonly-used grid-based representation. We show that, at the cost of longer exploration time, using the proposed representation allows full coverage of space even for very large drift in the state estimate, contrary to the grid-based representation. The system is validated in a real world experiment and we discuss its extension to 3D.&lt;/p></description></item><item><title>Real-time event simulation with frame-based cameras</title><link>https://andreasaziegler.github.io/publication/icra23-ziegler/</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/publication/icra23-ziegler/</guid><description/></item><item><title>Ball Detection with event-based asynchronous sparse convolutional networks</title><link>https://andreasaziegler.github.io/teaching/scnn/</link><pubDate>Sun, 21 Feb 2021 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/teaching/scnn/</guid><description>&lt;h2 id="description">Description&lt;/h2>
&lt;p>Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μs), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision.&lt;/p>
&lt;p>So far, most learning approaches applied to event data, convert a batch of events into a tensor and then use conventional CNNs as network. While such approaches achieve state-of-the-art performance, they do not make use of the asynchronous nature of the event data. Spiking Neural Networks (SNNs) on the other hand are bio-inspired networks that can process output from event-based directly. SNNs process information conveyed as temporal spikes rather than numeric values. This makes SNNs an ideal counterpart for event-based cameras.&lt;/p>
&lt;p>The goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls. The Cognitive Systems groups has a table tennis robot system, where the developed ball tracker can be used and compared to other methods.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>Familiar with &amp;ldquo;traditional&amp;rdquo; Computer Vision&lt;/li>
&lt;li>Deep Learning&lt;/li>
&lt;li>Python&lt;/li>
&lt;/ul></description></item><item><title>Spiking neural network for event-based ball detection</title><link>https://andreasaziegler.github.io/teaching/snn/</link><pubDate>Sun, 21 Feb 2021 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/teaching/snn/</guid><description>&lt;h2 id="description">Description&lt;/h2>
&lt;p>Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μs), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision.&lt;/p>
&lt;p>So far, most learning approaches applied to event data, convert a batch of events into a tensor and then use conventional CNNs as network. While such approaches achieve state-of-the-art performance, they do not make use of the asynchronous nature of the event data. Spiking Neural Networks (SNNs) on the other hand are bio-inspired networks that can process output from event-based directly. SNNs process information conveyed as temporal spikes rather than numeric values. This makes SNNs an ideal counterpart for event-based cameras.&lt;/p>
&lt;p>The goal of this thesis is to investigate and evaluate how a SNN can be used together with our event-based cameras to detect and track table tennis balls. The Cognitive Systems groups has a table tennis robot system, where the developed ball tracker can be used and compared to other methods.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>Familiar with &amp;ldquo;traditional&amp;rdquo; Computer Vision&lt;/li>
&lt;li>Deep Learning&lt;/li>
&lt;li>Python&lt;/li>
&lt;/ul></description></item><item><title>Exploration Without Global Consistency Using Local Volume Consolidation</title><link>https://andreasaziegler.github.io/publication/isrr19-cieslewski/</link><pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/publication/isrr19-cieslewski/</guid><description/></item><item><title>A Representation for Exploration that is Robust to State Estimate Drift</title><link>https://andreasaziegler.github.io/project/exploration/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/project/exploration/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Exploration is a fundamental task in the field of robotics. The goal is to build a map of a previously unknown environment. Two tasks have to be performed repeatedly for exploration: mapping the space the robot so far perceived; and planning where to go next. In this thesis we focus on the first task and the goal is to find a map representation that can deal with noisy state estimates. All so far presented map representations either assume perfect state estimates or need to rebuild the map after optimization.&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>To achieve this goal we build our representation with polygons. Our polygons represent the boundary between free known space and occupied space or unknown space and the inside of polygons is implicitly free space. We develop two approaches: The first approach builds a global map with polygons by continuously building the union of the polygon from the current field of view and the polygons of the so far explored space. The second approach works with polygons of the local field of view only. For every local polygon the frontiers, the obstacles and the free space is determined and the robot explores as long frontiers are present in the map.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;p>In this thesis we propose a novel representation that can deal with noisy state estimates and does not need to rebuild the map or parts of it, e.g. after a loop closure. By experiments we show that we achieve full coverage of the area to explore with frontier-based exploration, using our proposed representation.&lt;/p></description></item><item><title>Map Fusion for Collaborative UAV SLAM</title><link>https://andreasaziegler.github.io/project/map-fusion/</link><pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/project/map-fusion/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>A correct and accurate common map is crucial for multiple robots to collaboratively performing tasks. In this semester project, an existing multi agent Simultaneous Localisation and Mapping (SLAM) system should be extended to fuse maps of single robots in a way that no false map alignment is guaranteed and that an optimal alignment is achieved by using multiple place matches. Also redundant information, a consequence of the fusion of two maps, should be removed in order to get a good performance of the optimization routines e.g. Bundle Adjustment (BA).&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>To achieve the first goal, a new approach is proposed which uses multiple KeyFrame Matchs (KFMs) to fuse two maps. This proposed approach also use a novel idea to spread the KFMs over a bigger area by skipping KeyFrames (KFs) between the detection of KFMs. To remove redundant information, KF culling is performed after all KFMs were detected and before the main map fusion. This way information which were present in both maps appear only once in the fused map and the Pose Graph Optimization (PGO) and the BA does not have unnecessary data to process. To reduce the runtime of the optimization part (PGO and BA), the usage of Powell’s dog leg (DL) non-linear least squares technique instead of the Levenberg-Marquardt (LM) optimization was evaluated and the system was adapted in a way, that the performance is increased.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;p>The proposed map fusion approach reduces drift and achieves better accuracy compared to the previous approach. With the implemented KF culling, the number of KFs which the PGO and the BA have to process is decreased significantly and therefore better timing is achieved. The usage of DL non-linear least squares technique reduced the runtime of the optimization furthermore.&lt;/p></description></item></channel></rss>