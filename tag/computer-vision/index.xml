<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Vision | Andreas Ziegler</title><link>https://andreasaziegler.github.io/tag/computer-vision/</link><atom:link href="https://andreasaziegler.github.io/tag/computer-vision/index.xml" rel="self" type="application/rss+xml"/><description>Computer Vision</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Andreas Ziegler</copyright><lastBuildDate>Thu, 30 Mar 2017 00:00:00 +0000</lastBuildDate><image><url>https://andreasaziegler.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Computer Vision</title><link>https://andreasaziegler.github.io/tag/computer-vision/</link></image><item><title>Map Fusion for Collaborative UAV SLAM</title><link>https://andreasaziegler.github.io/project/map-fusion/</link><pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/project/map-fusion/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>A correct and accurate common map is crucial for multiple robots to collaboratively performing tasks. In this semester project, an existing multi agent Simultaneous Localisation and Mapping (SLAM) system should be extended to fuse maps of single robots in a way that no false map alignment is guaranteed and that an optimal alignment is achieved by using multiple place matches. Also redundant information, a consequence of the fusion of two maps, should be removed in order to get a good performance of the optimization routines e.g. Bundle Adjustment (BA).&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>To achieve the first goal, a new approach is proposed which uses multiple KeyFrame Matchs (KFMs) to fuse two maps. This proposed approach also use a novel idea to spread the KFMs over a bigger area by skipping KeyFrames (KFs) between the detection of KFMs. To remove redundant information, KF culling is performed after all KFMs were detected and before the main map fusion. This way information which were present in both maps appear only once in the fused map and the Pose Graph Optimization (PGO) and the BA does not have unnecessary data to process. To reduce the runtime of the optimization part (PGO and BA), the usage of Powell’s dog leg (DL) non-linear least squares technique instead of the Levenberg-Marquardt (LM) optimization was evaluated and the system was adapted in a way, that the performance is increased.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;p>The proposed map fusion approach reduces drift and achieves better accuracy compared to the previous approach. With the implemented KF culling, the number of KFs which the PGO and the BA have to process is decreased significantly and therefore better timing is achieved. The usage of DL non-linear least squares technique reduced the runtime of the optimization furthermore.&lt;/p></description></item><item><title>Robust object tracking in 3D by fusing ultra-wideband and vision</title><link>https://andreasaziegler.github.io/project/object-tracking/</link><pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate><guid>https://andreasaziegler.github.io/project/object-tracking/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Object tracking is an important part for many applications especially for robotic systems interacting with humans. Problem statement Ultra-wideband (UWB) systems as well as vision based object trackers are widely known and used. Both of the systems have their advantages and disadvantages. UWB systems can provide the location of an object in 3D with an accuracy of approximate 10cm whereas vision based object trackers can only provide the location of an object in 2D pixel coordinates but with a more precise accuracy than UWB systems.&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>So why not combine these two sources of information? Exactly this concept should be developed and evaluated in this semester project. The 3D position measured by the UWB system should be fused with the 2D pixel coordinates of a visual object tracker with an Extended Kalman Filter (EKF). A re-detection mechanism for the visual tracker should be implemented in addition to increase the usability as well as the stability of the system.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;p>The proposed system shows a significantly better accuracy compared with the 3D positions measured by the UWB system. This proof of concept enables to apply this system to a wide range of applications and also allows further extensions.&lt;/p></description></item></channel></rss>